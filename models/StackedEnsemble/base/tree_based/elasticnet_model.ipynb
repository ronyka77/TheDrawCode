{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model Implementation with CPU Optimization\n",
    "\n",
    "This notebook implements an XGBoost-based model for soccer match draw prediction with CPU optimization. The implementation includes:\n",
    "\n",
    "- Model creation and configuration\n",
    "- Training with early stopping\n",
    "- Threshold optimization\n",
    "- Hyperparameter tuning\n",
    "- Model evaluation\n",
    "- MLflow integration for experiment tracking\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Extended hypertune_elasticnet() function definition ---\n",
    "import mlflow\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, Tuple\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().absolute().parent.parent.parent.parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "os.environ[\"PYTHONPATH\"] = project_root + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "# Initialize the ExperimentLogger\n",
    "from utils.logger import ExperimentLogger\n",
    "logger = ExperimentLogger(\"elasticnet_hypertuning_extended\")\n",
    "\n",
    "from models.StackedEnsemble.meta_learners.elasticnet_model import ElasticNetModel\n",
    "\n",
    "def hypertune_elasticnet(X_train, y_train, X_test, y_test, X_val, y_val, logger):\n",
    "    \"\"\"\n",
    "    Hypertune the ElasticNet model over an extended hard-coded grid and log the best candidate using MLflow.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train, y_train: Training data.\n",
    "        X_test, y_test: Testing data.\n",
    "        X_val, y_val: Validation data.\n",
    "        logger: ExperimentLogger instance.\n",
    "    \n",
    "    Returns:\n",
    "        best_params: Dictionary of the best candidate hyperparameters.\n",
    "    \"\"\"\n",
    "    # Extended grid of candidate hyperparameters\n",
    "    candidate_alphas = [0.005, 0.01, 0.015, 0.02, 0.025]\n",
    "    candidate_l1_ratios = [0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "    candidate_max_iters = [150000, 200000, 250505]\n",
    "    candidate_poly_degrees = [1, 2, 3]\n",
    "    candidate_feature_selection_thresholds = [0.01, 0.014333562560261739, 0.02]\n",
    "    \n",
    "    best_precision = 0.0\n",
    "    best_params = None\n",
    "    target_precision = 0.45  # Set target precision threshold\n",
    "    \n",
    "    # Loop over all combinations in the extended grid\n",
    "    for alpha, l1_ratio, max_iter, poly_degree, feat_sel_thresh in product(\n",
    "            candidate_alphas,\n",
    "            candidate_l1_ratios,\n",
    "            candidate_max_iters,\n",
    "            candidate_poly_degrees,\n",
    "            candidate_feature_selection_thresholds):\n",
    "        \n",
    "        candidate_params = {\n",
    "            'alpha': alpha,\n",
    "            'alpha_grid_size': 170,  # Fixed value; adjust if needed\n",
    "            'l1_ratio': l1_ratio,\n",
    "            'max_iter': max_iter,\n",
    "            'tol': 4.836479789228169e-05,\n",
    "            'eps': 5.283984885624899e-05,\n",
    "            'positive': False,\n",
    "            'feature_selection_threshold': feat_sel_thresh,\n",
    "            'imputation_strategy': 'median',\n",
    "            'scaling_method': 'robust',\n",
    "            'poly_degree': poly_degree,\n",
    "            'random_seed': 42  # Fixed seed for reproducibility\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Evaluating candidate: alpha={alpha}, l1_ratio={l1_ratio}, \"\n",
    "                    f\"max_iter={max_iter}, poly_degree={poly_degree}, \"\n",
    "                    f\"feature_selection_threshold={feat_sel_thresh}\")\n",
    "        \n",
    "        # Instantiate the ElasticNetModel with the candidate parameters\n",
    "        model_instance = ElasticNetModel(\n",
    "            experiment_name='elasticnet_hypertune',\n",
    "            logger=logger,\n",
    "            random_seed=candidate_params['random_seed']\n",
    "        )\n",
    "        # Create the model using candidate parameters.\n",
    "        model_instance.model = model_instance._create_model(**candidate_params)\n",
    "        \n",
    "        try:\n",
    "            # Fit the model to obtain evaluation metrics.\n",
    "            # (Assume that the fit() method returns a dictionary with at least the 'precision' metric.)\n",
    "            metrics = model_instance.fit(X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "            candidate_precision = metrics.get('precision', 0)\n",
    "            logger.info(f\"Candidate precision: {candidate_precision:.4f}\")\n",
    "            \n",
    "            # Update best candidate if target precision is met and candidate's precision is highest so far.\n",
    "            if candidate_precision >= target_precision and candidate_precision > best_precision:\n",
    "                best_precision = candidate_precision\n",
    "                best_params = candidate_params\n",
    "                logger.info(f\"New best candidate found: {best_params} with precision {best_precision:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating candidate with alpha={alpha}, l1_ratio={l1_ratio}: {e}\")\n",
    "    \n",
    "    # Log the best candidate to MLflow if a valid candidate was found\n",
    "    if best_params is not None:\n",
    "        with mlflow.start_run(run_name=f\"elasticnet_hypertuning_best_{datetime.now().strftime('%Y%m%d_%H%M')}\"):\n",
    "            mlflow.log_params(best_params)\n",
    "            mlflow.log_metric(\"best_precision\", best_precision)\n",
    "            logger.info(\"Logged best candidate to MLflow.\")\n",
    "    else:\n",
    "        logger.error(\"No candidate met the target precision requirement.\")\n",
    "    \n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-20 23:17:54,062 | INFO     | elasticnet_hypertuning_extended | Loading data splits according to ensemble strategy\n",
      "2025-02-20 23:17:54,066 | INFO     | elasticnet_hypertuning_extended | Returning features common to all models\n",
      "2025-02-20 23:17:54,069 | INFO     | elasticnet_hypertuning_extended | Loaded 102 selected features\n",
      "2025-02-20 23:17:54,138 | INFO     | elasticnet_hypertuning_extended | Loaded training data from parquet: c:\\Users\\szita\\Documents\\TheDrawCode\\data\\api_training_final.parquet\n",
      "2025-02-20 23:17:54,275 | INFO     | elasticnet_hypertuning_extended | Loaded training/test data:\n",
      " - Training samples: 22343\n",
      " - Test samples: 5586\n",
      "2025-02-20 23:17:54,277 | INFO     | elasticnet_hypertuning_extended | Loading training data from: c:\\Users\\szita\\Documents\\TheDrawCode\\data\\prediction\\api_prediction_eval.xlsx\n",
      "2025-02-20 23:18:13,718 | INFO     | elasticnet_hypertuning_extended | Ensemble evaluation set created with shape: (3374, 200)\n",
      "2025-02-20 23:18:13,720 | INFO     | elasticnet_hypertuning_extended | Draw rate: 26.62%\n",
      "2025-02-20 23:18:13,723 | INFO     | elasticnet_hypertuning_extended | Train set shape: (3374, 199)\n",
      "2025-02-20 23:18:13,725 | INFO     | elasticnet_hypertuning_extended | Test set shape: (3374,)\n",
      "2025-02-20 23:18:13,729 | INFO     | elasticnet_hypertuning_extended | Loaded validation data: 3374 samples\n",
      "2025-02-20 23:18:13,750 | INFO     | elasticnet_hypertuning_extended | Final data split sizes:\n",
      " - Train: (22343, 102) (for model training and nested CV)\n",
      " - Test: (5586, 102) (for early stopping during training)\n",
      " - Validation: (3374, 102) (held-out for evaluation and meta-features)\n",
      "2025-02-20 23:18:13,752 | INFO     | elasticnet_hypertuning_extended | Evaluating candidate: alpha=0.005, l1_ratio=0.8, max_iter=150000, poly_degree=1, feature_selection_threshold=0.01\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m X_train, y_train, X_test, y_test, X_val, y_val \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Run the extended hypertuning function\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m best_candidate \u001b[38;5;241m=\u001b[39m \u001b[43mhypertune_elasticnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_candidate:\n\u001b[0;32m     15\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest candidate hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 82\u001b[0m, in \u001b[0;36mhypertune_elasticnet\u001b[1;34m(X_train, y_train, X_test, y_test, X_val, y_val, logger)\u001b[0m\n\u001b[0;32m     76\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m ElasticNetModel(\n\u001b[0;32m     77\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melasticnet_hypertune\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     78\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[0;32m     79\u001b[0m     random_seed\u001b[38;5;241m=\u001b[39mcandidate_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_seed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     80\u001b[0m )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Create the model using candidate parameters.\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m model_instance\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model_instance\u001b[38;5;241m.\u001b[39m_create_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcandidate_params)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Fit the model to obtain evaluation metrics.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# (Assume that the fit() method returns a dictionary with at least the 'precision' metric.)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m model_instance\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, X_test, y_test, X_val, y_val)\n",
      "File \u001b[1;32mc:\\Users\\szita\\Documents\\TheDrawCode\\models\\StackedEnsemble\\meta_learners\\elasticnet_model.py:134\u001b[0m, in \u001b[0;36mElasticNetModel._create_model\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_seed\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    133\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_seed)\n\u001b[1;32m--> 134\u001b[0m     \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_seed)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Get alpha value and create alphas array\u001b[39;00m\n\u001b[0;32m    137\u001b[0m alpha \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Example usage of the extended hypertune_elasticnet() function ---\n",
    "# Import data loading utilities\n",
    "from models.StackedEnsemble.shared.data_loader import DataLoader\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(experiment_name=\"elasticnet_model\")\n",
    "\n",
    "# Load train, test and validation data\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = data_loader.load_data()\n",
    "\n",
    "\n",
    "# Run the extended hypertuning function\n",
    "best_candidate = hypertune_elasticnet(X_train, y_train, X_test, y_test, X_val, y_val, logger)\n",
    "if best_candidate:\n",
    "    logger.info(f\"Best candidate hyperparameters: {best_candidate}\")\n",
    "else:\n",
    "    logger.error(\"Hypertuning did not find any valid candidate.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soccerpredictor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
