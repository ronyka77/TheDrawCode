{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model Implementation with CPU Optimization\n",
    "\n",
    "This notebook implements an XGBoost-based model for soccer match draw prediction with CPU optimization. The implementation includes:\n",
    "\n",
    "- Model creation and configuration\n",
    "- Training with early stopping\n",
    "- Threshold optimization\n",
    "- Hyperparameter tuning\n",
    "- Model evaluation\n",
    "- MLflow integration for experiment tracking\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 22:45:47,312 | INFO     | xgboost_soccer_prediction | Setting up MLflow tracking for experiment: xgboost_soccer_prediction\n",
      "mlflow local_path_uri: c:/Users/szita/Documents/TheDrawCode/mlruns\n",
      "2025-03-02 22:45:47,443 | INFO     | xgboost_soccer_prediction | Using existing experiment: xgboost_soccer_prediction experiment_id: 600562561289637747\n",
      "2025-03-02 22:45:47,447 | INFO     | xgboost_soccer_prediction | MLflow tracking configured successfully at: c:/Users/szita/Documents/TheDrawCode/mlruns\n",
      "2025-03-02 22:45:47,449 | INFO     | xgboost_soccer_prediction | Loading data splits according to ensemble strategy\n",
      "2025-03-02 22:45:47,453 | INFO     | xgboost_soccer_prediction | Returning features common to all models\n",
      "2025-03-02 22:45:47,455 | INFO     | xgboost_soccer_prediction | Loaded 102 selected features\n",
      "2025-03-02 22:45:47,540 | INFO     | xgboost_soccer_prediction | Loaded training data from parquet: c:\\Users\\szita\\Documents\\TheDrawCode\\data\\api_training_final.parquet\n",
      "2025-03-02 22:45:47,723 | INFO     | xgboost_soccer_prediction | Loaded training/test data:\n",
      " - Training samples: 22343\n",
      " - Test samples: 5586\n",
      "2025-03-02 22:45:47,729 | INFO     | xgboost_soccer_prediction | Loading training data from: c:\\Users\\szita\\Documents\\TheDrawCode\\data\\prediction\\api_prediction_eval.xlsx\n",
      "2025-03-02 22:46:17,592 | INFO     | xgboost_soccer_prediction | Ensemble evaluation set created with shape: (3667, 200)\n",
      "2025-03-02 22:46:17,595 | INFO     | xgboost_soccer_prediction | Draw rate: 26.18%\n",
      "2025-03-02 22:46:17,598 | INFO     | xgboost_soccer_prediction | Train set shape: (3667, 199)\n",
      "2025-03-02 22:46:17,600 | INFO     | xgboost_soccer_prediction | Test set shape: (3667,)\n",
      "2025-03-02 22:46:17,603 | INFO     | xgboost_soccer_prediction | Loaded validation data: 3667 samples\n",
      "2025-03-02 22:46:17,605 | INFO     | xgboost_soccer_prediction | Applying feature selection with consistent column ordering\n",
      "2025-03-02 22:46:17,713 | INFO     | xgboost_soccer_prediction | Replacing NaN values with 0 in all data splits\n",
      "2025-03-02 22:46:17,757 | INFO     | xgboost_soccer_prediction | Replacing inf values with 0 in all data splits\n",
      "2025-03-02 22:46:17,864 | INFO     | xgboost_soccer_prediction | NaN replacement complete:\n",
      " - Train: 0 NaN values replaced\n",
      " - Test: 0 NaN values replaced\n",
      " - Validation: 0 NaN values replaced\n",
      "2025-03-02 22:46:17,866 | INFO     | xgboost_soccer_prediction | Final data split sizes:\n",
      " - Train: (22343, 102) (for model training and nested CV)\n",
      " - Test: (5586, 102) (for early stopping during training)\n",
      " - Validation: (3667, 102) (held-out for evaluation and meta-features)\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Imports and Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import random\n",
    "from typing import Any, Dict, Tuple\n",
    "from datetime import datetime\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import yaml\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().absolute().parent.parent.parent.parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "os.environ[\"PYTHONPATH\"] = project_root + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"ARROW_S3_DISABLE\"] = \"1\"\n",
    "# Configure Git executable path if available\n",
    "git_executable = os.environ.get(\"GIT_PYTHON_GIT_EXECUTABLE\")\n",
    "if git_executable and os.path.exists(git_executable):\n",
    "    import git\n",
    "    git.refresh(git_executable)\n",
    "\n",
    "from utils.logger import ExperimentLogger\n",
    "experiment_name = \"xgboost_soccer_prediction\"\n",
    "logger = ExperimentLogger(experiment_name)\n",
    "\n",
    "from utils.create_evaluation_set import setup_mlflow_tracking\n",
    "from models.StackedEnsemble.utils.metrics import calculate_metrics\n",
    "from models.StackedEnsemble.shared.data_loader import DataLoader\n",
    "from models.ensemble.data_utils import balance_and_clean_dataset\n",
    "\n",
    "# Load data\n",
    "mlruns_dir = setup_mlflow_tracking(experiment_name)\n",
    "dataloader = DataLoader()\n",
    "X_train, y_train, X_test, y_test, X_eval, y_eval = dataloader.load_data()\n",
    "# X_train, y_train = balance_and_clean_dataset(X_train, y_train)\n",
    "# X_test, y_test = balance_and_clean_dataset(X_test, y_test)\n",
    "min_recall = 0.20\n",
    "n_trials = 500\n",
    "base_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': ['auc', 'logloss', 'error'],\n",
    "            'tree_method': 'hist',\n",
    "            'n_jobs': -1,\n",
    "            'verbosity': 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Configuration Loading\n",
    "def load_hyperparameter_space():\n",
    "    \"\"\"Define hyperparameter space for optimization.\"\"\"\n",
    "    try:\n",
    "        hyperparameter_space = {\n",
    "            'learning_rate': {\n",
    "                'type': 'float',\n",
    "                'low': 0.001,\n",
    "                'high': 0.05,\n",
    "                'log': True\n",
    "            },\n",
    "            'max_depth': {\n",
    "                'type': 'int', \n",
    "                'low': 4,\n",
    "                'high': 10\n",
    "            },\n",
    "            'min_child_weight': {\n",
    "                'type': 'int',\n",
    "                'low': 100,\n",
    "                'high': 400\n",
    "            },\n",
    "            'subsample': {\n",
    "                'type': 'float',\n",
    "                'low': 0.5,\n",
    "                'high': 1.0\n",
    "            },\n",
    "            'colsample_bytree': {\n",
    "                'type': 'float',\n",
    "                'low': 0.6,\n",
    "                'high': 1.0\n",
    "            },\n",
    "            'reg_alpha': {\n",
    "                'type': 'float',\n",
    "                'low': 0.001,\n",
    "                'high': 10.0,\n",
    "                'log': True\n",
    "            },\n",
    "            'reg_lambda': {\n",
    "                'type': 'float',\n",
    "                'low': 5.0,\n",
    "                'high': 20.0,\n",
    "                'log': True\n",
    "            },\n",
    "            'gamma': {\n",
    "                'type': 'float',\n",
    "                'low': 0.5,\n",
    "                'high': 7.0\n",
    "            },\n",
    "            'early_stopping_rounds': {\n",
    "                'type': 'int',\n",
    "                'low': 300,\n",
    "                'high': 1000\n",
    "            },\n",
    "            'scale_pos_weight': {\n",
    "                'type': 'float',\n",
    "                'low': 3.0,\n",
    "                'high': 7.0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        \n",
    "        return hyperparameter_space\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating hyperparameter space: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "hyperparameter_space = load_hyperparameter_space()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Model Creation\n",
    "def create_model(model_params):\n",
    "    \"\"\"Create and configure XGBoost model instance.\"\"\"\n",
    "    try:\n",
    "        params = base_params\n",
    "        \n",
    "        # Update with provided parameters\n",
    "        params.update(model_params)\n",
    "        \n",
    "        # Create model\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating XGBoost model: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Data Conversion\n",
    "def convert_to_model_format(X: pd.DataFrame, y: pd.Series = None):\n",
    "    \"\"\"Convert data to XGBoost DMatrix format.\"\"\"\n",
    "    if X is None:\n",
    "        raise ValueError(\"The feature dataset X must not be None.\")\n",
    "    \n",
    "    try:\n",
    "        if y is not None:\n",
    "            dmatrix = xgb.DMatrix(X, label=y)\n",
    "        else:\n",
    "            dmatrix = xgb.DMatrix(X)\n",
    "        return dmatrix\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting data to DMatrix: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Prediction Functions\n",
    "def predict(model, X, threshold=0.5):\n",
    "    \"\"\"Generate predictions using trained model.\"\"\"\n",
    "    if model is None:\n",
    "        raise RuntimeError(\"Model must be trained before prediction\")\n",
    "        \n",
    "    try:\n",
    "        probas = model.predict_proba(X)[:, 1]\n",
    "        return (probas >= threshold).astype(int)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in model prediction: {str(e)}\")\n",
    "        return np.zeros(len(X))\n",
    "\n",
    "def predict_proba(model, X):\n",
    "    \"\"\"Generate probability predictions.\"\"\"\n",
    "    if model is None:\n",
    "        raise RuntimeError(\"Model must be trained before prediction\")\n",
    "        \n",
    "    try:\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in probability prediction: {str(e)}\")\n",
    "        return np.zeros(len(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X: Any, y: Any, best_threshold: float) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate model performance on given data.\"\"\"\n",
    "    if model is None:\n",
    "        raise RuntimeError(\"Model must be trained before evaluation\")\n",
    "    \n",
    "    try:\n",
    "        # Get probability predictions\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Get binary predictions using best threshold\n",
    "        y_pred = (y_prob >= best_threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = np.sum((y == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y == 1) & (y_pred == 0))\n",
    "        \n",
    "        metrics = {\n",
    "            'precision': tp / (tp + fp + 1e-10),\n",
    "            'recall': tp / (tp + fn + 1e-10),\n",
    "            'f1': 2 * tp / (2 * tp + fp + fn + 1e-10),\n",
    "            'auc': roc_auc_score(y, y_prob),\n",
    "            'brier_score': np.mean((y_prob - y) ** 2),\n",
    "            'threshold': best_threshold\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in model evaluation: {str(e)}\")\n",
    "        return {\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0,\n",
    "            'auc': 0.0,\n",
    "            'brier_score': 1.0,\n",
    "            'threshold': best_threshold\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold(model, y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    \"\"\"Optimize prediction threshold with focus on precision while maintaining recall above 15%.\"\"\"\n",
    "    try:\n",
    "        best_threshold = 0.5\n",
    "        best_precision = 0.0\n",
    "        \n",
    "        # Search through thresholds\n",
    "        for threshold in np.linspace(0.3, 0.8, 51):\n",
    "            y_pred = (y_prob >= threshold).astype(int)\n",
    "            \n",
    "            # Calculate confusion matrix components\n",
    "            tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "            fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "            fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "            \n",
    "            precision = tp / (tp + fp + 1e-10)\n",
    "            recall = tp / (tp + fn + 1e-10)\n",
    "            \n",
    "            # Only consider thresholds that maintain recall above 15%\n",
    "            if recall >= min_recall:\n",
    "                if precision > best_precision:\n",
    "                    best_precision = precision\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        logger.info(f\"Optimized threshold: {best_threshold:.3f} with precision: {best_precision:.3f}\")\n",
    "        metrics = evaluate(model, X_eval, y_eval, best_threshold)\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error optimizing threshold: {str(e)}\")\n",
    "        return 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Training Function\n",
    "def train_model(X_train, y_train, X_test, y_test, X_eval, y_eval, model_params):\n",
    "    \"\"\"Train XGBoost model with early stopping.\"\"\"\n",
    "    try:\n",
    "        # Create model with remaining parameters\n",
    "        model = create_model(model_params)\n",
    "        \n",
    "        # Create eval set for early stopping\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        \n",
    "        # Fit model with early stopping\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=eval_set,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Get validation predictions\n",
    "        y_prob = model.predict_proba(X_eval)[:, 1]\n",
    "        metrics = optimize_threshold(model, y_eval, y_prob)\n",
    "        \n",
    "        return model, metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training XGBoost model: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Model Save/Load\n",
    "def save_model(model, path, threshold=None):\n",
    "    \"\"\"Save XGBoost model and threshold to specified path.\"\"\"\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Save model\n",
    "        joblib.dump(model, path)\n",
    "        \n",
    "        # Save threshold\n",
    "        if threshold:\n",
    "            threshold_path = path.parent / \"threshold.json\"\n",
    "            with open(threshold_path, 'w') as f:\n",
    "                json.dump({\n",
    "                    'threshold': threshold,\n",
    "                    'model_type': 'xgboost',\n",
    "                    'params': model.get_params()\n",
    "                }, f, indent=2)\n",
    "                \n",
    "        logger.info(f\"Model saved to {path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_model(path):\n",
    "    \"\"\"Load XGBoost model from specified path.\"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No model file found at {path}\")\n",
    "        \n",
    "    try:\n",
    "        # Load model\n",
    "        model = joblib.load(path)\n",
    "        \n",
    "        # Load threshold\n",
    "        threshold_path = path.parent / \"threshold.json\"\n",
    "        if threshold_path.exists():\n",
    "            with open(threshold_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                threshold = data.get('threshold', 0.5)\n",
    "        else:\n",
    "            threshold = 0.5\n",
    "            \n",
    "        logger.info(f\"Model loaded from {path}\")\n",
    "        return model, threshold\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: Hyperparameter Tuning\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "    try:\n",
    "        params = base_params\n",
    "        \n",
    "        # Add hyperparameters from config\n",
    "        hyperparameter_space = load_hyperparameter_space()\n",
    "        for param_name, param_config in hyperparameter_space.items():\n",
    "            if param_config['type'] == 'float':\n",
    "                params[param_name] = trial.suggest_float(\n",
    "                    param_name,\n",
    "                    param_config['low'], \n",
    "                    param_config['high'],\n",
    "                    log=param_config.get('log', False)  # Pass log if in param_config, default False\n",
    "                )\n",
    "            elif param_config['type'] == 'int':\n",
    "                params[param_name] = trial.suggest_int(\n",
    "                    param_name,\n",
    "                    param_config['low'],\n",
    "                    param_config['high']\n",
    "                )\n",
    "            elif param_config['type'] == 'categorical':\n",
    "                params[param_name] = trial.suggest_categorical(\n",
    "                    param_name,\n",
    "                    param_config['choices']\n",
    "                )\n",
    "        # Train model and get metrics\n",
    "        model, metrics = train_model(\n",
    "            X_train, y_train,\n",
    "            X_test, y_test,\n",
    "            X_eval, y_eval,\n",
    "            params\n",
    "        )\n",
    "        \n",
    "        recall = metrics.get('recall', 0.0)\n",
    "        precision = metrics.get('precision', 0.0)\n",
    "        \n",
    "        # Report intermediate values for pruning\n",
    "        # trial.report(precision, step=1)\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.TrialPruned()\n",
    "        \n",
    "        # Optimize for precision while maintaining minimum recall\n",
    "        score = precision if recall >= min_recall else 0.0\n",
    "        \n",
    "        logger.info(f\"Trial {trial.number}:\")\n",
    "        logger.info(f\"  Params: {params}\")\n",
    "        logger.info(f\"  Score: {score}\")\n",
    "        \n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            trial.set_user_attr(metric_name, metric_value)\n",
    "        return score\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in trial {trial.number}: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 9: Hypertuning Function\n",
    "def hypertune_xgboost(experiment_name: str) -> float:\n",
    "    \"\"\"Run hyperparameter optimization with MLflow tracking.\"\"\"\n",
    "    try:\n",
    "        # Create study\n",
    "        study = optuna.create_study(\n",
    "            study_name=f\"xgboost_optimization_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "            direction=\"maximize\",\n",
    "            sampler = optuna.samplers.TPESampler(                     # Different seed for better randomization\n",
    "                    n_startup_trials=20,         # Reduced from 50 - more efficient\n",
    "                    prior_weight=0.4\n",
    "                )\n",
    "            # pruner = optuna.pruners.HyperbandPruner(  # CHANGE: Better than MedianPruner for XGBoost\n",
    "            #     min_resource=1,              # Minimum number of training steps\n",
    "            #     max_resource=10,             # Maximum number of training iterations to consider\n",
    "            #     reduction_factor=3           # Controls aggressiveness of pruning\n",
    "            # )\n",
    "        )\n",
    "        \n",
    "        # Start MLflow run\n",
    "        with mlflow.start_run(run_name=f\"xgboost_optimization_{datetime.now().strftime('%Y%m%d_%H%M')}\"):\n",
    "            # Log dataset info\n",
    "            mlflow.log_params({\n",
    "                \"train_samples\": len(X_train),\n",
    "                \"test_samples\": len(X_test),\n",
    "                \"eval_samples\": len(X_eval),\n",
    "                \"features\": X_train.shape[1]\n",
    "            })\n",
    "            \n",
    "            # Set tags\n",
    "            mlflow.set_tags({\n",
    "                \"model_type\": \"xgboost_base\",\n",
    "                \"optimization\": \"optuna\",\n",
    "                \"cpu_only\": True\n",
    "            })\n",
    "            \n",
    "            # Optimize\n",
    "            best_score = -float('inf')  # Initialize with worst possible score\n",
    "            best_params = {}\n",
    "            def callback(study, trial):\n",
    "                nonlocal best_score\n",
    "                nonlocal best_params\n",
    "                logger.info(f\"Current best score: {best_score:.4f}\")\n",
    "                if trial.value > best_score:\n",
    "                    best_score = trial.value\n",
    "                    best_params = trial.params\n",
    "                    logger.info(f\"New best score found in trial {trial.number}: {best_score:.4f}\")\n",
    "                return best_score\n",
    "            \n",
    "            study.optimize(objective, n_trials=n_trials, \n",
    "                            timeout=10000, show_progress_bar = True, callbacks=[callback])  # 2 hours timeout\n",
    "            best_params.update(**base_params)\n",
    "            # Log best trial info\n",
    "            logger.info(f\"Best trial value: {best_score}\")\n",
    "            logger.info(f\"Best Parameters: {best_params}\")\n",
    "            logger.info(\"Training final model with best parameters\")\n",
    "            final_model, final_metrics = train_model(\n",
    "                X_train, y_train,\n",
    "                X_test, y_test,\n",
    "                X_eval, y_eval,\n",
    "                best_params\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Training completed with precision: {final_metrics['precision']:.4f}\")\n",
    "            return final_metrics['precision'], best_params\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in hyperparameter optimization: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_precision_target(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    X_eval: np.ndarray,\n",
    "    y_eval: np.ndarray,\n",
    "    logger: ExperimentLogger) -> Tuple[Any, float, Dict[str, Any]]:\n",
    "    \"\"\"Train XGBoost model with target precision threshold.\"\"\"\n",
    "    \n",
    "    precision = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_params = None\n",
    "    best_seed = 0\n",
    "    best_model = None\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    # Base parameters from previous optimization\n",
    "    params = {\n",
    "        'tree_method': 'hist',  # Required for CPU-only training per project rules\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'aucpr', \n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': 0.023063607691447444,\n",
    "        'max_depth': 10,\n",
    "        'min_child_weight': 100,\n",
    "        'subsample': 0.4118438147817545, \n",
    "        'colsample_bytree': 0.7517099195364563,\n",
    "        'reg_alpha': 0.031549778623225215,\n",
    "        'reg_lambda': 7.079771696753476,\n",
    "        'gamma': 0.8306756479665651,\n",
    "        'early_stopping_rounds': 316,\n",
    "        'scale_pos_weight': 2.52367628440707,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    while best_precision < 0.48:  # Target precision threshold\n",
    "        for random_seed in range(1, 800):  # Try up to 1000 different seeds\n",
    "            logger.info(f\"Using sequential random seed: {random_seed}\")\n",
    "            \n",
    "            # Set all random seeds\n",
    "            os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "            np.random.seed(random_seed)\n",
    "            random.seed(random_seed)\n",
    "            params['random_state'] = random_seed\n",
    "            \n",
    "            try:\n",
    "                # Create and train model\n",
    "                model, metrics = train_model(\n",
    "                    X_train, y_train,\n",
    "                    X_test, y_test,\n",
    "                    X_eval, y_eval,\n",
    "                    **params\n",
    "                )\n",
    "                precision = metrics['precision']\n",
    "                recall = metrics['recall']\n",
    "\n",
    "                # Update best model if precision improved\n",
    "                if precision > best_precision:\n",
    "                    best_precision = precision\n",
    "                    best_recall = recall\n",
    "                    best_params = params.copy()\n",
    "                    best_seed = random_seed\n",
    "                    best_model = model\n",
    "                    logger.info(f\"New best precision: {precision:.4f} with seed {best_seed}\")\n",
    "                \n",
    "                # Check if target precision reached\n",
    "                if precision >= 0.48:\n",
    "                    logger.info(f\"Target precision achieved: {precision:.4f}\")\n",
    "                    return best_model, precision, recall, best_params\n",
    "                \n",
    "                logger.info(\n",
    "                    f\"Current precision: {precision:.4f}, \"\n",
    "                    f\"target: 0.4800, highest precision: {best_precision:.4f}, \"\n",
    "                    f\"best seed: {best_seed}\"\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error training with seed {random_seed}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # Clear model to free memory\n",
    "            model = None\n",
    "        \n",
    "        # If target not reached after all seeds, return best model\n",
    "        if precision < 0.48:\n",
    "            logger.info(f\"Target precision not reached, using best seed: {best_seed}\")\n",
    "            return best_model, best_precision, best_recall, best_params\n",
    "            \n",
    "    return best_model, best_precision, best_recall, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_mlflow(model: object, precision: float, recall: float, params: dict, experiment_name: str) -> str:\n",
    "    \"\"\"Log model, metrics and parameters to MLflow.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained XGBoost model\n",
    "        metrics (dict): Dictionary of metrics like precision, recall etc.\n",
    "        params (dict): Model parameters used for training\n",
    "        experiment_name (str): Name of the MLflow experiment\n",
    "    \"\"\"\n",
    "    from utils.create_evaluation_set import setup_mlflow_tracking\n",
    "    \n",
    "    mlruns_dir = setup_mlflow_tracking(experiment_name)\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=f\"xgboost_base_{datetime.now().strftime('%Y%m%d_%H%M')}\") as run:\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        })\n",
    "        \n",
    "        # Register model with timestamp\n",
    "        model_name = f\"xgboost_base_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "        \n",
    "        # Log model with signature\n",
    "        # Create proper input example as DataFrame with single row\n",
    "        input_example = pd.DataFrame(X_train.iloc[0]).T\n",
    "        \n",
    "        # Get prediction as array for signature\n",
    "        pred = predict(model, input_example)\n",
    "        signature = mlflow.models.infer_signature(\n",
    "            model_input=input_example,\n",
    "            model_output=pred\n",
    "        )\n",
    "        \n",
    "        mlflow.xgboost.log_model(\n",
    "            xgb_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=model_name,\n",
    "            signature=signature\n",
    "        )\n",
    "        \n",
    "        # Log run ID\n",
    "        run_id = run.info.run_id\n",
    "        logger.info(f\"Run ID: {run_id}\")\n",
    "        return run_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seed_model():\n",
    "    model, precision, recall, best_params = train_with_precision_target(\n",
    "                X_train, y_train,\n",
    "                X_test, y_test,\n",
    "                X_eval, y_eval,\n",
    "                logger\n",
    "            )\n",
    "    print(f\"Training completed with precision: {precision:.4f}\")\n",
    "    \n",
    "    # Log to MLflow if we got a valid model\n",
    "    if model is not None:\n",
    "        log_to_mlflow(model, precision, recall, best_params, experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 22:46:18,845 | INFO     | xgboost_soccer_prediction | Starting hypertuning run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 22:46:18,849] A new study created in memory with name: xgboost_optimization_20250302_2246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ccfaf2742f411fa089c0682d5dad0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 22:46:22,224 | INFO     | xgboost_soccer_prediction | Optimized threshold: 0.700 with precision: 0.340\n",
      "2025-03-02 22:46:22,278 | INFO     | xgboost_soccer_prediction | Trial 0:\n",
      "2025-03-02 22:46:22,284 | INFO     | xgboost_soccer_prediction |   Params: {'objective': 'binary:logistic', 'eval_metric': ['auc', 'logloss', 'error'], 'tree_method': 'hist', 'n_jobs': -1, 'verbosity': 0, 'learning_rate': 0.008992711236478315, 'max_depth': 4, 'min_child_weight': 140, 'subsample': 0.7614330188267323, 'colsample_bytree': 0.9521778408204591, 'reg_alpha': 0.0025740742375474027, 'reg_lambda': 14.275779930124505, 'gamma': 0.5544819531149592, 'early_stopping_rounds': 898, 'scale_pos_weight': 6.058064369752594}\n",
      "2025-03-02 22:46:22,290 | INFO     | xgboost_soccer_prediction |   Score: 0.3403416557161182\n",
      "[I 2025-03-02 22:46:22,297] Trial 0 finished with value: 0.3403416557161182 and parameters: {'learning_rate': 0.008992711236478315, 'max_depth': 4, 'min_child_weight': 140, 'subsample': 0.7614330188267323, 'colsample_bytree': 0.9521778408204591, 'reg_alpha': 0.0025740742375474027, 'reg_lambda': 14.275779930124505, 'gamma': 0.5544819531149592, 'early_stopping_rounds': 898, 'scale_pos_weight': 6.058064369752594}. Best is trial 0 with value: 0.3403416557161182.\n",
      "2025-03-02 22:46:22,305 | INFO     | xgboost_soccer_prediction | Current best score: -inf\n",
      "2025-03-02 22:46:22,321 | ERROR    | xgboost_soccer_prediction | Error in hyperparameter optimization: 'FrozenTrial' object has no attribute 'score'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FrozenTrial' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[328], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      9\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting hypertuning run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     precision, params \u001b[38;5;241m=\u001b[39m \u001b[43mhypertune_xgboost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed with precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Track the best run\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[324], line 50\u001b[0m, in \u001b[0;36mhypertune_xgboost\u001b[1;34m(experiment_name)\u001b[0m\n\u001b[0;32m     47\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew best score found in trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;241m.\u001b[39mnumber\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_score\n\u001b[1;32m---> 50\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 2 hours timeout\u001b[39;00m\n\u001b[0;32m     52\u001b[0m best_params\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbase_params)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Log best trial info\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\szita\\.conda\\envs\\soccerpredictor_env\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\szita\\.conda\\envs\\soccerpredictor_env\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\szita\\.conda\\envs\\soccerpredictor_env\\lib\\site-packages\\optuna\\study\\_optimize.py:171\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[1;32m--> 171\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrozen_trial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     elapsed_seconds \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m time_start)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "Cell \u001b[1;32mIn[324], line 44\u001b[0m, in \u001b[0;36mhypertune_xgboost.<locals>.callback\u001b[1;34m(study, trial)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m best_params\n\u001b[0;32m     43\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent best score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m \u001b[38;5;241m>\u001b[39m best_score:\n\u001b[0;32m     45\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mscore\n\u001b[0;32m     46\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mparams\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FrozenTrial' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # train_seed_model()\n",
    "    # Run multiple hypertuning sessions and track the best precision\n",
    "    best_precision = 0\n",
    "    best_params = {}\n",
    "    best_run_id = None\n",
    "    \n",
    "    for i in range(5):\n",
    "        logger.info(f\"Starting hypertuning run {i+1}/5\")\n",
    "        precision, params = hypertune_xgboost(experiment_name)\n",
    "        \n",
    "        logger.info(f\"Run {i+1} completed with precision: {precision:.4f}\")\n",
    "        \n",
    "        # Track the best run\n",
    "        if precision > best_precision:\n",
    "            best_precision = precision\n",
    "            best_params = params\n",
    "    \n",
    "    logger.info(f\"Best precision: {best_precision:.4f} and best Parameters {best_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soccerpredictor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
