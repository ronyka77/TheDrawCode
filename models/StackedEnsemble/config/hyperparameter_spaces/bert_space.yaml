hyperparameters:
  # Model architecture parameters
  hidden_dropout_prob:
    distribution: uniform
    min: 0.1
    max: 0.3
  
  attention_probs_dropout_prob:
    distribution: uniform
    min: 0.1
    max: 0.3
  
  # Training parameters
  learning_rate:
    distribution: log_uniform
    min: 5.0e-6
    max: 1.0e-4
  
  per_device_train_batch_size:
    distribution: int_uniform
    min: 4
    max: 16
  
  gradient_accumulation_steps:
    distribution: int_uniform
    min: 2
    max: 8
  
  num_train_epochs:
    distribution: int_uniform
    min: 2
    max: 4
  
  warmup_ratio:
    distribution: uniform
    min: 0.05
    max: 0.15
  
  weight_decay:
    distribution: log_uniform
    min: 0.005
    max: 0.05
  
  # Sequence parameters
  max_seq_length:
    distribution: int_uniform
    min: 128
    max: 384
  
  # Fixed parameters for CPU training
  device: cpu
  optim: adamw_torch
  lr_scheduler_type: linear
  max_grad_norm: 1.0
  
  # Model configuration
  model_name: bert-base-uncased
  num_labels: 2
  problem_type: single_label_classification

search_strategy:
  name: bayesian
  settings:
    max_trials: 20
    metric: precision
    mode: max
    early_stopping:
      type: asha
      grace_period: 100
      reduction_factor: 3
      max_t: 1000
      brackets: 1
    
  resources:
    cpu_per_trial: 1
    max_concurrent_trials: 1
    
  stopping:
    max_iterations: 1000
    target_metrics:
      precision: 0.35
      recall: 0.15
    
  checkpointing:
    frequency: 1
    keep_top_n: 2 