# Model Configuration
general:
  random_seed: 42
  experiment_name: "TheDrawCode"
  model_artifacts:
    save_format: "joblib"
    version_control: true
    monitoring:
      track_predictions: true
      track_feature_drift: true
      track_model_drift: true
      track_class_distribution: true

# Training configuration
training:
  batch_size: 256
  epochs: 50
  early_stopping: true
  patience: 5
  validation_split: 0.2
  shuffle: true
  random_seed: 42
  
  optimizer:
    name: "adam"
    learning_rate: 0.001
    weight_decay: 0.0001
    
  scheduler:
    name: "reduce_on_plateau"
    patience: 5
    factor: 0.5
    
  regularization:
    dropout_rate: 0.2
    l1_lambda: 0.0001
    l2_lambda: 0.0001

# Model parameters
model_params:
  xgboost:
    model_type: "binary_classification"
    params:
      objective: "binary:logistic"
      eval_metric: ["auc", "logloss", "error"]
      booster: "gbtree"
      eta: 0.01
      max_depth: 5
      min_child_weight: 6
      colsample_bytree: 0.4
      subsample: 0.5
      lambda: 5.0
      alpha: 4.0
      n_estimators: 3000
      early_stopping_rounds: 300
      scale_pos_weight: 10.0  # More aggressive weight for positive class
      max_delta_step: 5  # More controlled updates
      min_split_loss: 3.0  # Higher minimum loss reduction
      max_leaves: 16  # More restricted tree structure
      grow_policy: "lossguide"
      tree_method: "hist"
      sampling_method: "gradient_based"
    tuning_ranges:
      eta: [0.01, 0.3]
      max_depth: [4, 10]
      min_child_weight: [1, 10]

  catboost:
    model_type: "binary_classification"
    params:
      loss_function: "Logloss"
      eval_metric: "Precision"
      iterations: 5000
      learning_rate: 0.001
      depth: 8
      l2_leaf_reg: 40.0
      border_count: 254
      bootstrap_type: "Bayesian"
      bagging_temperature: 0.1
      random_strength: 0.1
      class_weights: [1.0, 6.0]  # Increased weight for draws
      verbose: 100
      early_stopping_rounds: 400
      leaf_estimation_iterations: 10
      leaf_estimation_method: "Newton"
      min_data_in_leaf: 50
      max_leaves: 64
      score_function: "Cosine"
      grow_policy: "Lossguide"
    tuning_ranges:
      learning_rate: [0.001, 0.1]
      depth: [4, 10]
      l2_leaf_reg: [1, 40]

  tabnet:
    model_type: "binary_classification"
    params:
      n_d: 16
      n_a: 16
      n_steps: 5
      gamma: 1.5
      lambda_sparse: 0.001
      optimizer_fn: "adam"
      n_classes: 2  # Binary classification
      optimizer_params:
        lr: 0.02
      mask_type: "entmax"
      scheduler_params:
        step_size: 10
        gamma: 0.9
      verbose: 1
      class_weights: [1.0, 2.846]  # [non-draw, draw]
    tuning_ranges:
      n_d: [8, 64]
      n_a: [8, 64]
      n_steps: [3, 10]
      gamma: [1.0, 2.0]
      lambda_sparse: [1e-5, 1e-2]

  tabtransformer:
    model_type: "binary_classification"
    params:
      num_layers: 3
      num_heads: 8
      num_classes: 2  # Binary classification
      hidden_dim: 128
      dropout: 0.1
      learning_rate: 0.001
      min_delta: 0.001
      batch_size: 1024
      epochs: 50
      early_stopping: true
      patience: 8
      weight_decay: 0.001
      min_lr: 1e-6
      eps: 1e-8
      factor: 0.5
      class_weights: [1.0, 2.846]  # [non-draw, draw]
    tuning_ranges:
      num_layers: [2, 6]
      num_heads: [4, 16]
      hidden_dim: [64, 256]
      dropout: [0.1, 0.5]

  gnn:
    model_type: "binary_classification"
    params:
      hidden_dim: 128
      num_layers: 2
      dropout: 0.2
      learning_rate: 0.001
      batch_size: 256
      epochs: 50
      output_dim: 1
      sequence_length: 5
      aggregator_type: "mean"
      class_weights: [1.0, 2.846]
    tuning_ranges:
      hidden_dim: [64, 256]
      num_layers: [2, 4]
      dropout: [0.1, 0.5]
      aggregator_type: ["mean", "max", "lstm"]

  # Updated sequence model configuration
  sequence_model:
    model_type: "binary_classification"
    params:
      type: "lstm"
      input_dim: null  # Will be set dynamically
      hidden_dim: 128
      num_layers: 2
      dropout: 0.2
      bidirectional: true
      output_dim: 1
      batch_size: 64
      epochs: 50
      learning_rate: 0.001
      patience: 5
      weight_decay: 0.01
      min_lr: 1e-6
      eps: 1e-8
      factor: 0.5
      class_weights: [1.0, 2.846]
    tuning_ranges:
      hidden_dim: [64, 256]
      num_layers: [1, 3]
      dropout: [0.1, 0.5]

  # Added calibration model configuration
  calibration_model:
    model_type: "calibration"
    params:
      type: "isotonic"
      cv_folds: 5
      threshold_range: [0.1, 0.9]
      threshold_steps: 100
      scoring: "f1"
      target_precision: 0.50
      target_recall: 0.60

  threshold_optimization:
    model_type: "threshold_optimization"
    params:
      type: "precision_recall_curve"
      target_precision: 0.50
      target_recall: 0.60
      step_size: 0.001
      optimization_metric: "f1"
      search_range: [0.1, 0.9]
      n_thresholds: 1000
      validation_metric: "f1"
      early_stopping: true
      patience: 5

  pnn:
    model_type: "binary_classification"
    params:
      sigma: 0.02
      batch_size: 128
      class_weights: [1.0, 1.8]
      temperature: 0.15
      min_distance_weight: 0.02
      kernel_params:
        adaptive_sigma_factor: 0.2
        min_sigma: 0.01
        max_sigma: 0.05
      threshold_params:
        initial_threshold: 0.55
        min_predictions: 40
        precision_target: 0.45
        recall_target: 0.55
      knn_params:
        k: 50
        min_ratio: 1.2
    tuning_ranges:
      sigma: [0.01, 0.05]
      batch_size: [64, 256]
      temperature: [0.1, 0.2]

  hme:
    model_type: "binary_classification"
    params:
      n_experts: 3
      hidden_dim: 64
      learning_rate: 0.001
      batch_size: 64
      epochs: 100
      class_weights: [1.0, 2.846]  # [non-draw, draw]
    tuning_ranges:
      n_experts: [2, 5]
      hidden_dim: [32, 128]
      learning_rate: [0.0001, 0.01]
      batch_size: [32, 256]
      epochs: [50, 200]

  stacked_model:
    epochs: 150
    batch_size: 64
    learning_rate: 0.002
    dropout_rate: 0.25
    hidden_dim: 128
    num_layers: 3

  voting_ensemble:
    min_votes: 2
    base_confidence_threshold: 0.6
    agreement_bonus: 1.2
    min_recall: 0.3
    optimization:
      threshold_range: [0.3, 0.8]
      threshold_steps: 50
      min_samples_per_league: 50
      precision_weight: 0.7
      recall_weight: 0.3
      fbeta: 2
    weights:
      precision_importance: 0.7
      recall_importance: 0.3
    league_specific:
      enable_thresholds: true
      enable_weights: true
      min_league_samples: 50
    model_weights:
      catboost: 1.0
      xgboost: 1.0
      stacked: 1.0

# Evaluation settings
evaluation:
  metrics:
    - "precision"
    - "recall"
    - "f1"
    - "average_precision"
    - "precision_at_threshold"
    - "recall_at_threshold"
    - "confusion_matrix"
    - "classification_report"
  threshold_optimization: true
  calibration_plot: true
  feature_importance: true
  metrics_format: "json"
  cross_validation:
    n_folds: 5
    stratification: true
    shuffle: true
  target_metrics:
    precision: 0.50
    recall: 0.60
  threshold_metrics:
    precision_at_threshold: 0.50
    recall_at_threshold: 0.60
    f1_at_threshold: null

# Class weights for binary classification
class_weights:
  non_draw_weight: 1.0
  draw_weight: 4.0  # Increased draw weight

# Model monitoring
monitoring:
  track_predictions: true
  track_feature_drift: true
  track_model_drift: true
  track_class_distribution: true
  prediction_logging: true
  performance_tracking: true
  drift_thresholds:
    feature_drift: 0.1
    prediction_drift: 0.1
    class_distribution_drift: 0.05

# Removed unused models and simplified configuration